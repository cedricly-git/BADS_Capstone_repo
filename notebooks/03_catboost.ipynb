{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost Model\n\nThis notebook builds and evaluates a **CatBoost** model to predict `estimated_daily_searches` using weather and temporal features.\n\n**Focus**: High performance with SHAP interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport requests\nfrom datetime import datetime\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\nfrom catboost import CatBoostRegressor\nimport shap\nimport pickle\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\nplt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UberEats Data Shape: (728, 10)\n",
      "\n",
      "Fetching weather data for 10 cities...\n",
      "  \u2713 Zurich\n",
      "  \u2713 Geneva\n",
      "  \u2713 Basel\n",
      "  \u2713 Lausanne\n",
      "  \u2713 Bern\n",
      "  \u2713 Winterthur\n",
      "  \u2713 Lucerne\n",
      "  \u2713 St. Gallen\n",
      "  \u2713 Lugano\n",
      "  \u2713 Biel\n",
      "Dropped column: weight\n",
      "Dropped column: trend_value\n",
      "Dropped column: total_ubereats\n",
      "\n",
      "Merged Dataset Shape: (728, 14)\n",
      "Date range: 2023-11-13 to 2025-11-11\n",
      "Missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# Load UberEats data\n",
    "url = \"https://raw.githubusercontent.com/cedricly-git/BADS_Capstone_repo/main/Data/ubereats+time_related_vars.csv\"\n",
    "uber = pd.read_csv(url)\n",
    "uber['Day'] = pd.to_datetime(uber['Day'])\n",
    "\n",
    "print(\"UberEats Data Shape:\", uber.shape)\n",
    "\n",
    "# Load weather data for top 10 most populated cities in Switzerland\n",
    "cities = {\n",
    "    \"Zurich\": {\"lat\": 47.3769, \"lon\": 8.5417, \"pop\": 436551},\n",
    "    \"Geneva\": {\"lat\": 46.2044, \"lon\": 6.1432, \"pop\": 209061},\n",
    "    \"Basel\": {\"lat\": 47.5596, \"lon\": 7.5886, \"pop\": 177571},\n",
    "    \"Lausanne\": {\"lat\": 46.5197, \"lon\": 6.6323, \"pop\": 144873},\n",
    "    \"Bern\": {\"lat\": 46.9481, \"lon\": 7.4474, \"pop\": 137995},\n",
    "    \"Winterthur\": {\"lat\": 47.5056, \"lon\": 8.7247, \"pop\": 120376},\n",
    "    \"Lucerne\": {\"lat\": 47.0502, \"lon\": 8.3064, \"pop\": 86234},\n",
    "    \"St. Gallen\": {\"lat\": 47.4245, \"lon\": 9.3767, \"pop\": 78863},\n",
    "    \"Lugano\": {\"lat\": 46.0101, \"lon\": 8.9600, \"pop\": 63629},\n",
    "    \"Biel\": {\"lat\": 47.1404, \"lon\": 7.2471, \"pop\": 56896}\n",
    "}\n",
    "\n",
    "# Calculate population weights\n",
    "total_pop = sum(city[\"pop\"] for city in cities.values())\n",
    "city_weights = {name: city[\"pop\"] / total_pop for name, city in cities.items()}\n",
    "\n",
    "start_date = \"2023-11-13\"\n",
    "end_date = \"2025-11-11\"\n",
    "\n",
    "weather_data = []\n",
    "print(f\"\\nFetching weather data for {len(cities)} cities...\")\n",
    "for city, coords in cities.items():\n",
    "    url1 = (\n",
    "        f\"https://archive-api.open-meteo.com/v1/archive?\"\n",
    "        f\"latitude={coords['lat']}&longitude={coords['lon']}\"\n",
    "        f\"&start_date={start_date}&end_date={end_date}\"\n",
    "        f\"&daily=temperature_2m_max,temperature_2m_min,precipitation_sum&timezone=Europe/Zurich\"\n",
    "    )\n",
    "    data = requests.get(url1).json()\n",
    "    for i, date in enumerate(data['daily']['time']):\n",
    "        weather_data.append({\n",
    "            \"Day\": datetime.strptime(date, \"%Y-%m-%d\"),\n",
    "            \"City\": city,\n",
    "            \"Temp_Max\": data['daily']['temperature_2m_max'][i],\n",
    "            \"Temp_Min\": data['daily']['temperature_2m_min'][i],\n",
    "            \"Precipitation\": data['daily']['precipitation_sum'][i]\n",
    "        })\n",
    "    print(f\"  \u2713 {city}\")\n",
    "\n",
    "weather_df = pd.DataFrame(weather_data)\n",
    "\n",
    "# Calculate population-weighted average weather\n",
    "weather_df['pop_weight'] = weather_df['City'].map(city_weights)\n",
    "weather_avg = weather_df.groupby('Day').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Temp_Max': (x['Temp_Max'] * x['pop_weight']).sum(),\n",
    "        'Temp_Min': (x['Temp_Min'] * x['pop_weight']).sum(),\n",
    "        'Precipitation': (x['Precipitation'] * x['pop_weight']).sum()\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Merge datasets\n",
    "df = pd.merge(uber, weather_avg, on='Day', how='left')\n",
    "\n",
    "# Add derived features\n",
    "df[\"temp_range\"] = df[\"Temp_Max\"] - df[\"Temp_Min\"]\n",
    "df[\"dayofweek\"] = df[\"Day\"].dt.weekday\n",
    "df[\"month_num\"] = df[\"Day\"].dt.month\n",
    "df[\"year\"] = df[\"Day\"].dt.year\n",
    "\n",
    "# Drop columns that shouldn't be used as features (they were used to create estimated_daily_searches)\n",
    "columns_to_drop = ['weight', 'trend_value', 'total_uber', 'total_ubereats']\n",
    "for col in columns_to_drop:\n",
    "    if col in df.columns:\n",
    "        df = df.drop(columns=[col])\n",
    "        print(f\"Dropped column: {col}\")\n",
    "\n",
    "print(f\"\\nMerged Dataset Shape: {df.shape}\")\n",
    "print(f\"Date range: {df['Day'].min().date()} to {df['Day'].max().date()}\")\n",
    "print(f\"Missing values: {df.isnull().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape after feature engineering: (718, 35)\n",
      "\n",
      "New features created:\n",
      "- Cyclical encoding: dayofweek_sin/cos, month_sin/cos\n",
      "- Weather derived: temp_comfort, precip_binary, precip_heavy\n",
      "- Lag features (lag1): Temp_Max_lag1, Temp_Min_lag1, Precipitation_lag1, estimated_daily_searches_lag1\n",
      "- Extended lag features (lag7): estimated_daily_searches, Temp_Max, Temp_Min, Precipitation\n",
      "- Rolling averages: Temp_Max_7d, Precipitation_7d\n",
      "- Polynomial features: Temp_Max_squared\n",
      "- Interaction features: Temp_Max_weekend, Precipitation_weekend, temp_comfort_weekend\n"
     ]
    }
   ],
   "source": [
    "# Cyclical encoding for temporal features (better than linear encoding)\n",
    "df['dayofweek_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)\n",
    "df['dayofweek_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month_num'] / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month_num'] / 12)\n",
    "\n",
    "# Weather-derived features\n",
    "df['temp_comfort'] = (df['Temp_Max'] + df['Temp_Min']) / 2  # Average temperature\n",
    "df['precip_binary'] = (df['Precipitation'] > 0).astype(int)  # Rain/no rain\n",
    "df['precip_heavy'] = (df['Precipitation'] > 10).astype(int)  # Heavy rain flag\n",
    "\n",
    "# Lag features (previous day weather - may affect today's searches)\n",
    "df['Temp_Max_lag1'] = df['Temp_Max'].shift(1)\n",
    "df['Temp_Min_lag1'] = df['Temp_Min'].shift(1)\n",
    "df['Precipitation_lag1'] = df['Precipitation'].shift(1)\n",
    "df['estimated_daily_searches_lag1'] = df['estimated_daily_searches'].shift(1)\n",
    "\n",
    "# Extended lag features (lag7 for weekly patterns - lag14 removed to preserve more data)\n",
    "df['estimated_daily_searches_lag7'] = df['estimated_daily_searches'].shift(7)\n",
    "df['Temp_Max_lag7'] = df['Temp_Max'].shift(7)\n",
    "df['Temp_Min_lag7'] = df['Temp_Min'].shift(7)\n",
    "df['Precipitation_lag7'] = df['Precipitation'].shift(7)\n",
    "\n",
    "# Rolling averages (smooth out daily fluctuations)\n",
    "df['Temp_Max_7d'] = df['Temp_Max'].rolling(window=7, center=True).mean()\n",
    "df['Precipitation_7d'] = df['Precipitation'].rolling(window=7, center=True).mean()\n",
    "\n",
    "# Polynomial features (capture non-linear temperature effects)\n",
    "df['Temp_Max_squared'] = df['Temp_Max'] ** 2  # Optimal temperature range effect\n",
    "\n",
    "# Interaction features (weather \u00d7 temporal interactions)\n",
    "# These capture how weather effects differ on weekends/holidays\n",
    "df['Temp_Max_weekend'] = df['Temp_Max'] * df['is_weekend']\n",
    "df['Precipitation_weekend'] = df['Precipitation'] * df['is_weekend']\n",
    "df['temp_comfort_weekend'] = df['temp_comfort'] * df['is_weekend']\n",
    "\n",
    "# Drop rows with NaN from lag/rolling features\n",
    "df = df.dropna()\n",
    "\n",
    "print(f\"Dataset shape after feature engineering: {df.shape}\")\n",
    "print(f\"\\nNew features created:\")\n",
    "print(\"- Cyclical encoding: dayofweek_sin/cos, month_sin/cos\")\n",
    "print(\"- Weather derived: temp_comfort, precip_binary, precip_heavy\")\n",
    "print(\"- Lag features (lag1): Temp_Max_lag1, Temp_Min_lag1, Precipitation_lag1, estimated_daily_searches_lag1\")\n",
    "print(\"- Extended lag features (lag7): estimated_daily_searches, Temp_Max, Temp_Min, Precipitation\")\n",
    "print(\"- Rolling averages: Temp_Max_7d, Precipitation_7d\")\n",
    "print(\"- Polynomial features: Temp_Max_squared\")\n",
    "print(\"- Interaction features: Temp_Max_weekend, Precipitation_weekend, temp_comfort_weekend\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train-Test Split (Time-Series Aware)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 574 days (2023-11-20 to 2025-06-17)\n",
      "Test set: 144 days (2025-06-18 to 2025-11-08)\n",
      "\n",
      "Available features: 27\n",
      "['is_weekend', 'is_holiday', 'dayofweek_sin', 'dayofweek_cos', 'month_sin', 'month_cos', 'Temp_Max', 'Temp_Min', 'Precipitation', 'temp_range', 'temp_comfort', 'precip_binary', 'precip_heavy', 'Temp_Max_lag1', 'Temp_Min_lag1', 'Precipitation_lag1', 'estimated_daily_searches_lag1', 'estimated_daily_searches_lag7', 'Temp_Max_lag7', 'Temp_Min_lag7', 'Precipitation_lag7', 'Temp_Max_7d', 'Precipitation_7d', 'Temp_Max_squared', 'Temp_Max_weekend', 'Precipitation_weekend', 'temp_comfort_weekend']\n",
      "\n",
      "Feature matrix shape - Train: (574, 27), Test: (144, 27)\n"
     ]
    }
   ],
   "source": [
    "# Sort by date to ensure proper time-series split\n",
    "df = df.sort_values('Day').reset_index(drop=True)\n",
    "\n",
    "# Time-based split (last 20% for testing)\n",
    "split_idx = int(len(df) * 0.8)\n",
    "train_df = df.iloc[:split_idx].copy()\n",
    "test_df = df.iloc[split_idx:].copy()\n",
    "\n",
    "print(f\"Training set: {len(train_df)} days ({train_df['Day'].min().date()} to {train_df['Day'].max().date()})\")\n",
    "print(f\"Test set: {len(test_df)} days ({test_df['Day'].min().date()} to {test_df['Day'].max().date()})\")\n",
    "\n",
    "# Define target and features\n",
    "target = 'estimated_daily_searches'\n",
    "\n",
    "# Feature selection - interpretable features for linear models\n",
    "feature_candidates = [\n",
    "    # Temporal\n",
    "    'is_weekend', 'is_holiday', 'dayofweek_sin', 'dayofweek_cos', \n",
    "    'month_sin', 'month_cos',\n",
    "    # Weather\n",
    "    'Temp_Max', 'Temp_Min', 'Precipitation', 'temp_range', 'temp_comfort',\n",
    "    'precip_binary', 'precip_heavy',\n",
    "    # Lag features (lag1)\n",
    "    'Temp_Max_lag1', 'Temp_Min_lag1', 'Precipitation_lag1', 'estimated_daily_searches_lag1',\n",
    "    # Extended lag features (lag7)\n",
    "    'estimated_daily_searches_lag7',\n",
    "    'Temp_Max_lag7', 'Temp_Min_lag7', 'Precipitation_lag7',\n",
    "    # Rolling averages\n",
    "    'Temp_Max_7d', 'Precipitation_7d',\n",
    "    # Polynomial features\n",
    "    'Temp_Max_squared',\n",
    "    # Interaction features\n",
    "    'Temp_Max_weekend', 'Precipitation_weekend', 'temp_comfort_weekend'\n",
    "]\n",
    "\n",
    "# Check which features exist\n",
    "available_features = [f for f in feature_candidates if f in df.columns]\n",
    "print(f\"\\nAvailable features: {len(available_features)}\")\n",
    "print(available_features)\n",
    "\n",
    "X_train = train_df[available_features]\n",
    "y_train = train_df[target]\n",
    "X_test = test_df[available_features]\n",
    "y_test = test_df[target]\n",
    "\n",
    "print(f\"\\nFeature matrix shape - Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# Optional: Feature scaling (helps with regularization, but makes coefficients less interpretable)\n",
    "# For interpretability, we'll skip scaling, but models with regularization can benefit\n",
    "# Uncomment below if you want to use scaling:\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "# X_train = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "# X_test = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CatBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\ndef evaluate_model(y_true, y_pred, model_name):\n    \"\"\"Calculate and display multiple evaluation metrics\"\"\"\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    mae = mean_absolute_error(y_true, y_pred)\n    mape = mean_absolute_percentage_error(y_true, y_pred) * 100\n    r2 = r2_score(y_true, y_pred)\n    \n    print(f\"\\n{model_name} Performance:\")\n    print(\"=\" * 50)\n    print(f\"RMSE:  {rmse:.2f}\")\n    print(f\"MAE:   {mae:.2f}\")\n    print(f\"MAPE:  {mape:.2f}%\")\n    print(f\"R\u00b2:    {r2:.4f}\")\n    \n    return {'RMSE': rmse, 'MAE': mae, 'MAPE': mape, 'R\u00b2': r2}\n\n# CatBoost - Gradient boosting that handles categorical features natively\nprint(\"=\" * 60)\nprint(\"CatBoost Regression\")\nprint(\"=\" * 60)\n\n# Identify categorical features\ncategorical_features = ['is_weekend', 'is_holiday', 'precip_binary', 'precip_heavy']\ncat_features_indices = [i for i, col in enumerate(X_train.columns) if col in categorical_features]\n\nprint(f\"Categorical features for CatBoost: {[X_train.columns[i] for i in cat_features_indices]}\")\n\n# CatBoost model with early stopping\ncatboost = CatBoostRegressor(\n    iterations=1000,\n    learning_rate=0.05,\n    depth=8,\n    loss_function='RMSE',\n    eval_metric='RMSE',\n    random_seed=42,\n    verbose=False,\n    cat_features=cat_features_indices if cat_features_indices else None,\n    l2_leaf_reg=3,\n    border_count=128\n)\n\n# Fit with early stopping\ncatboost.fit(\n    X_train, y_train,\n    eval_set=(X_test, y_test),\n    early_stopping_rounds=50,\n    verbose=False\n)\n\ny_pred_catboost = catboost.predict(X_test)\nresults_catboost = evaluate_model(y_test, y_pred_catboost, \"CatBoost\")\n\n# Feature importance\ncatboost_importance = pd.DataFrame({\n    'feature': X_train.columns,\n    'importance': catboost.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(\"\\nTop 15 Most Important Features (CatBoost):\")\nprint(catboost_importance.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SHAP Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP (SHapley Additive exPlanations) - Explain CatBoost predictions\nprint(\"=\" * 60)\nprint(\"SHAP Interpretation for CatBoost\")\nprint(\"=\" * 60)\n\n# Create SHAP explainer (use a sample for faster computation)\nprint(\"\\nComputing SHAP values (this may take a minute)...\")\nsample_size = min(100, len(X_train))\nX_train_sample = X_train.iloc[:sample_size]\n\nexplainer = shap.TreeExplainer(catboost)\nshap_values = explainer.shap_values(X_train_sample)\n\nprint(f\"SHAP values computed for {sample_size} samples\")\n\n# Summary Plot - Feature importance\nprint(\"\\nSHAP Summary Plot (Feature Importance & Impact)\")\nshap.summary_plot(shap_values, X_train_sample, plot_type=\"bar\", show=False)\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model, predictions, and results\nos.makedirs('models', exist_ok=True)\nos.makedirs('results', exist_ok=True)\n\n# Save model\nwith open('models/catboost.pkl', 'wb') as f:\n    pickle.dump(catboost, f)\n\n# Save predictions and results\ncatboost_results = {\n    'model_name': 'CatBoost',\n    'predictions': y_pred_catboost.tolist(),\n    'metrics': results_catboost,\n    'test_dates': test_df['Day'].dt.strftime('%Y-%m-%d').tolist(),\n    'y_test': y_test.tolist()\n}\n\nwith open('results/catboost_results.pkl', 'wb') as f:\n    pickle.dump(catboost_results, f)\n\nprint(\"Model and results saved successfully!\")\nprint(f\"Model saved to: models/catboost.pkl\")\nprint(f\"Results saved to: results/catboost_results.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Actual vs Predicted\nplt.figure(figsize=(10, 8))\nplt.scatter(y_test, y_pred_catboost, alpha=0.6, s=50)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2, label='Perfect Prediction')\nplt.xlabel('Actual Estimated Daily Searches', fontsize=12)\nplt.ylabel('Predicted Estimated Daily Searches', fontsize=12)\nplt.title(f'CatBoost: Actual vs Predicted (R\u00b2 = {results_catboost[\"R\u00b2\"]:.4f})', fontsize=14, fontweight='bold')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}